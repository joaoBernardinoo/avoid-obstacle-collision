{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define as dimensões da imagem como referência\n",
    "IMG_HEIGHT = 40\n",
    "IMG_WIDTH = 200\n",
    "IMG_CHANNELS = 4  # BGRA do Webots\n",
    "\n",
    "class CNNNavigationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a arquitetura de um modelo CNN robusto para navegação usando dados de câmera e LiDAR.\n",
    "\n",
    "    Este modelo aprimorado calcula dinamicamente o tamanho da entrada para as camadas\n",
    "    totalmente conectadas, tornando-o robusto a mudanças nas dimensões da imagem de entrada\n",
    "    ou na arquitetura da CNN. Ele também refatora a ramificação da câmera para maior clareza\n",
    "    e inclui inicialização de pesos Kaiming.\n",
    "    \"\"\"\n",
    "    def __init__(self, lidar_shape_in, img_shape=(IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH)):\n",
    "        \"\"\"\n",
    "        Inicializa o modelo CNN.\n",
    "\n",
    "        Args:\n",
    "            lidar_shape_in (int): O número de características de entrada para a ramificação LiDAR.\n",
    "            img_shape (tuple): A forma da imagem de entrada (C, H, W).\n",
    "        \"\"\"\n",
    "        super(CNNNavigationModel, self).__init__()\n",
    "\n",
    "        # --- Ramificação da Câmera ---_\n",
    "        # Definimos a parte convolucional primeiro para calcular dinamicamente seu tamanho de saída.\n",
    "        camera_conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=img_shape[0], out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # **MELHORIA 1: Cálculo dinâmico do tamanho das características**\n",
    "        # Criamos um tensor \"dummy\" e o passamos pelas camadas convolucionais para descobrir o tamanho achatado.\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *img_shape)\n",
    "            flattened_cam_size = camera_conv_layers(dummy_input).shape[1]\n",
    "            print(f\"Tamanho da característica da câmera achatada calculado dinamicamente: {flattened_cam_size}\")\n",
    "\n",
    "\n",
    "        # **MELHORIA 2: Clareza arquitetural**\n",
    "        # Combinamos todo o pipeline de processamento da câmera em um único módulo sequencial.\n",
    "        self.camera_branch = nn.Sequential(\n",
    "            camera_conv_layers,\n",
    "            nn.Linear(flattened_cam_size, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- Ramificação do LiDAR ---_\n",
    "        self.lidar_branch = nn.Sequential(\n",
    "            nn.Linear(lidar_shape_in, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- Cabeça Combinada ---_\n",
    "        # Esta parte permanece conceitualmente a mesma.\n",
    "        self.combined_head = nn.Sequential(\n",
    "            nn.Linear(64 + 64, 128),  # 64 da câmera + 64 do LiDAR\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),  # Saída de 2 valores (dist, angulo)\n",
    "            nn.Tanh()          # Escala a saída para [-1, 1] para alvos normalizados\n",
    "        )\n",
    "\n",
    "        # **MELHORIA 3: Inicialização de Pesos**\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Aplica a inicialização Kaiming He às camadas Conv2d e Linear.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, cam_input, lidar_input):\n",
    "        \"\"\"\n",
    "        Define a passagem para a frente (forward pass) do modelo.\n",
    "\n",
    "        Args:\n",
    "            cam_input (torch.Tensor): O tensor de entrada da câmera.\n",
    "                                      Forma: (N, C, H, W)\n",
    "            lidar_input (torch.Tensor): O tensor de entrada do LiDAR.\n",
    "                                        Forma: (N, lidar_shape_in)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: O tensor de saída do modelo. Forma: (N, 2)\n",
    "        \"\"\"\n",
    "        cam_features = self.camera_branch(cam_input)\n",
    "        lidar_features = self.lidar_branch(lidar_input)\n",
    "        combined_features = torch.cat((cam_features, lidar_features), dim=1)\n",
    "        output = self.combined_head(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Define o backend não-interativo explicitamente\n",
    "\n",
    "# --- Configuration ---_\n",
    "SCRIPT_DIR = Path.cwd()\n",
    "DATASET_PATH = SCRIPT_DIR / 'cnn_dataset.h5'\n",
    "# Diretório para salvar os checkpoints\n",
    "CHECKPOINT_DIR = SCRIPT_DIR / 'checkpoints'\n",
    "PLOT_SAVE_PATH = SCRIPT_DIR / 'training_history.png'\n",
    "\n",
    "# Cria o diretório de checkpoints se ele não existir\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 40\n",
    "IMG_WIDTH = 200\n",
    "IMG_CHANNELS = 4  # BGRA from Webots\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "N_SPLITS = 5  # Número de folds para K-Fold\n",
    "\n",
    "# --- Device Configuration ---_\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(path: Path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Carrega o dataset de um único arquivo HDF5.\n",
    "    \"\"\"\n",
    "    if not path.exists() or not path.is_file():\n",
    "        print(f\"Error: HDF5 file not found at '{path}'\")\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        if 'camera_image' not in hf:\n",
    "            print(f\"Error: 'camera_image' dataset not found in '{path}'\")\n",
    "            raise ValueError\n",
    "\n",
    "        print(f\"Loading samples from '{path}'...\")\n",
    "        X_cam = np.array(hf['camera_image'], dtype=np.float32)\n",
    "        X_lidar = np.array(hf['lidar_data'], dtype=np.float32)\n",
    "        dist = np.array(hf['dist'], dtype=np.float32)\n",
    "        angle = np.array(hf['angle'], dtype=np.float32)\n",
    "        y = np.stack((dist, angle), axis=1)\n",
    "\n",
    "        if max_samples:\n",
    "            X_cam = X_cam[:max_samples]\n",
    "            X_lidar = X_lidar[:max_samples]\n",
    "            y = y[:max_samples]\n",
    "\n",
    "    # Normaliza imagens\n",
    "    X_cam /= 255.0\n",
    "\n",
    "    # --- NORMALIZE TARGETS (y) ---_\n",
    "    y[:, 0] /= 3.14  # Normalize distance\n",
    "    y[:, 1] /= np.pi   # Normalize angle\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "\n",
    "    # Normaliza dados do LiDAR\n",
    "    if X_lidar.size > 0:\n",
    "        max_lidar_val = np.max(X_lidar[np.isfinite(X_lidar)])\n",
    "        if max_lidar_val > 0:\n",
    "            X_lidar[np.isinf(X_lidar)] = max_lidar_val\n",
    "            X_lidar /= max_lidar_val\n",
    "\n",
    "    # Permuta os eixos da imagem para o formato do PyTorch (N, C, H, W)\n",
    "    X_cam = np.transpose(X_cam, (0, 3, 1, 2))\n",
    "\n",
    "    return (X_cam, X_lidar), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plota o histórico de treinamento e validação e salva em um arquivo.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(history['loss'], label='Training Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    ax2.plot(history['mae'], label='Training MAE')\n",
    "    ax2.plot(history['val_mae'], label='Validation MAE')\n",
    "    ax2.set_title('Model MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_SAVE_PATH)  # Salva a figura em um arquivo\n",
    "    plt.close(fig)  # Fecha a figura para liberar memória\n",
    "    print(f\"Training plot saved to '{PLOT_SAVE_PATH}'\")\n",
    "    plt.close(fig)  # Fecha a figura para liberar memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samples from '/home/dino/Documents/ia/controllers/cnn/cnn_dataset.h5'...\n"
     ]
    }
   ],
   "source": [
    "(X_cam, X_lidar), y = load_data(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samples from '/home/dino/Documents/ia/controllers/cnn/test.h5'...\n"
     ]
    }
   ],
   "source": [
    "(X_cam_test, X_lidar_test), y_test = load_data(Path('/home/dino/Documents/ia/controllers/cnn/test.h5'))  # /home/dino/Documents/ia/controllers/cnn/test.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max distance: -0.189000204205513\n",
      "Max angle: -0.3189796805381775\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shapes: Cam=(7493, 4, 40, 200), Lidar=(7493, 20), Target=(7493, 2)\n",
      "\n",
      "--- Starting Fold 1/5 ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Resuming training from checkpoint: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_1.pth\n",
      "Resumed from epoch 40. Best validation loss so far: 0.0031\n",
      "\n",
      "--- Starting Model Training for Fold 1 from Epoch 41 ---\n",
      "\n",
      "--- Starting Fold 2/5 ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Resuming training from checkpoint: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_2.pth\n",
      "Resumed from epoch 39. Best validation loss so far: 0.0022\n",
      "\n",
      "--- Starting Model Training for Fold 2 from Epoch 40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 Epoch 40/40 - Loss: 0.0032, MAE: 0.0372 - Val Loss: 0.0026, Val MAE: 0.0288\n",
      "\n",
      "--- Starting Fold 3/5 ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Resuming training from checkpoint: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_3.pth\n",
      "Resumed from epoch 40. Best validation loss so far: 0.0030\n",
      "\n",
      "--- Starting Model Training for Fold 3 from Epoch 41 ---\n",
      "\n",
      "--- Starting Fold 4/5 ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Resuming training from checkpoint: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_4.pth\n",
      "Resumed from epoch 36. Best validation loss so far: 0.0029\n",
      "\n",
      "--- Starting Model Training for Fold 4 from Epoch 37 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 Epoch 37/40 - Loss: 0.0035, MAE: 0.0395 - Val Loss: 0.0032, Val MAE: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 Epoch 38/40 - Loss: 0.0034, MAE: 0.0383 - Val Loss: 0.0030, Val MAE: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 Epoch 39/40 - Loss: 0.0034, MAE: 0.0385 - Val Loss: 0.0030, Val MAE: 0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 Epoch 40/40 - Loss: 0.0032, MAE: 0.0375 - Val Loss: 0.0029, Val MAE: 0.0282\n",
      "  -> New best validation loss: 0.0029. Saving checkpoint...\n",
      "\n",
      "--- Starting Fold 5/5 ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Resuming training from checkpoint: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_5.pth\n",
      "Resumed from epoch 39. Best validation loss so far: 0.0022\n",
      "\n",
      "--- Starting Model Training for Fold 5 from Epoch 40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 Epoch 40/40 - Loss: 0.0029, MAE: 0.0351 - Val Loss: 0.0021, Val MAE: 0.0254\n",
      "  -> New best validation loss: 0.0021. Saving checkpoint...\n",
      "Training plot saved to '/home/dino/Documents/ia/controllers/cnn/training_history.png'\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if X_cam is None or X_cam.shape[0] == 0:\n",
    "    print(\"Exiting: No data loaded.\")\n",
    "else:\n",
    "    print(f\"Data loaded. Shapes: Cam={X_cam.shape}, Lidar={X_lidar.shape}, Target={y.shape}\")\n",
    "\n",
    "    kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    fold_histories = []\n",
    "\n",
    "    best_overall_val_loss = float('inf')\n",
    "    best_overall_checkpoint_path = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_cam)):\n",
    "        print(f\"\\n--- Starting Fold {fold+1}/{N_SPLITS} ---\")\n",
    "\n",
    "        # Define o caminho do checkpoint para este fold específico\n",
    "        checkpoint_path = CHECKPOINT_DIR / f'checkpoint_fold_{fold+1}.pth'\n",
    "\n",
    "        # Split data for this fold\n",
    "        X_cam_train, y_train = X_cam[train_idx], y[train_idx]\n",
    "        X_lidar_train = X_lidar[train_idx]\n",
    "\n",
    "        X_cam_val, y_val = X_cam[val_idx], y[val_idx]\n",
    "        X_lidar_val = X_lidar[val_idx]\n",
    "\n",
    "        # Converte dados para tensores PyTorch\n",
    "        train_dataset = TensorDataset(torch.tensor(\n",
    "            X_cam_train), torch.tensor(X_lidar_train), torch.tensor(y_train))\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(\n",
    "            X_cam_val), torch.tensor(X_lidar_val), torch.tensor(y_val))\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Instancia o modelo e o otimizador\n",
    "        model = CNNNavigationModel(lidar_shape_in=X_lidar.shape[1]).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Inicializa variáveis para o loop de treino e checkpoint\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "        history = {'loss': [], 'val_loss': [], 'mae': [],\n",
    "                   'val_mae': [], \"best_val_mae\": 0.0}\n",
    "\n",
    "        # --- LÓGICA PARA CARREGAR CHECKPOINT ---_\n",
    "        if checkpoint_path.exists():\n",
    "            print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            best_val_loss = checkpoint['best_val_loss']\n",
    "            history = checkpoint['history']\n",
    "            print(\n",
    "                f\"Resumed from epoch {start_epoch}. Best validation loss so far: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"No checkpoint found for fold {fold+1}. Starting from scratch.\")\n",
    "\n",
    "        # Define as funções de perda\n",
    "        criterion = nn.MSELoss()\n",
    "        mae_criterion = nn.L1Loss()\n",
    "\n",
    "        print(\n",
    "            f\"\\n--- Starting Model Training for Fold {fold+1} from Epoch {start_epoch+1} ---\")\n",
    "        for epoch in range(start_epoch, EPOCHS):\n",
    "            # --- Fase de Treinamento ---_\n",
    "            model.train()\n",
    "            running_loss, running_mae = 0.0, 0.0\n",
    "            progress_bar = tqdm(\n",
    "                train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{EPOCHS} [T]\", leave=False)\n",
    "            for cam_batch, lidar_batch, target_batch in progress_bar:\n",
    "                cam_batch, lidar_batch, target_batch = cam_batch.to(\n",
    "                    device), lidar_batch.to(device), target_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(cam_batch, lidar_batch)\n",
    "                loss = criterion(outputs, target_batch)\n",
    "                mae = mae_criterion(outputs, target_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                running_mae += mae.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), mae=mae.item())\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_mae = running_mae / len(train_loader)\n",
    "            history['loss'].append(epoch_loss)\n",
    "            history['mae'].append(epoch_mae)\n",
    "\n",
    "            # --- Fase de Validação ---_\n",
    "            model.eval()\n",
    "            val_loss, val_mae = 0.0, 0.0\n",
    "            with torch.no_grad():\n",
    "                for cam_batch, lidar_batch, target_batch in val_loader:\n",
    "                    cam_batch, lidar_batch, target_batch = cam_batch.to(\n",
    "                        device), lidar_batch.to(device), target_batch.to(device)\n",
    "                    outputs = model(cam_batch, lidar_batch)\n",
    "                    val_loss += criterion(outputs, target_batch).item()\n",
    "                    val_mae += mae_criterion(outputs, target_batch).item()\n",
    "\n",
    "            epoch_val_loss = val_loss / len(val_loader)\n",
    "            epoch_val_mae = val_mae / len(val_loader)\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_mae'].append(epoch_val_mae)\n",
    "\n",
    "            print(\n",
    "                f\"Fold {fold+1}/{N_SPLITS} Epoch {epoch+1}/{EPOCHS} - \"\n",
    "                f\"Loss: {epoch_loss:.4f}, MAE: {epoch_mae:.4f} - \"\n",
    "                f\"Val Loss: {epoch_val_loss:.4f}, Val MAE: {epoch_val_mae:.4f}\"\n",
    "            )\n",
    "\n",
    "            # --- LÓGICA PARA SALVAR CHECKPOINT ---_\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                print(\n",
    "                    f\"  -> New best validation loss: {best_val_loss:.4f}. Saving checkpoint...\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'best_val_mae': epoch_val_mae,\n",
    "                    'history': history\n",
    "                }, checkpoint_path)\n",
    "\n",
    "                history['best_val_mae'] = epoch_val_mae\n",
    "        fold_histories.append(history)\n",
    "\n",
    "        # Update best overall checkpoint if this fold performed better\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            current_fold_best_val_loss = checkpoint['best_val_loss']\n",
    "            if current_fold_best_val_loss < best_overall_val_loss:\n",
    "                best_overall_val_loss = current_fold_best_val_loss\n",
    "                best_overall_checkpoint_path = checkpoint_path\n",
    "\n",
    "    # Agrega e plota os resultados de todos os folds\n",
    "    if fold_histories:\n",
    "        avg_history = {\n",
    "            'loss': np.mean([h['loss'] for h in fold_histories], axis=0).tolist(),\n",
    "            'val_loss': np.mean([h['val_loss'] for h in fold_histories], axis=0).tolist(),\n",
    "            'mae': np.mean([h['mae'] for h in fold_histories], axis=0).tolist(),\n",
    "            'val_mae': np.mean([h['val_mae'] for h in fold_histories], axis=0).tolist(),\n",
    "            'best_val_mae': np.mean([h['best_val_mae'] for h in fold_histories]).tolist()\n",
    "        }\n",
    "        plot_history(avg_history)\n",
    "\n",
    "    print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Fine-tuning on Entire Dataset ---\n",
      "Loading best model from: /home/dino/Documents/ia/controllers/cnn/checkpoints/checkpoint_fold_5.pth\n",
      "Loading samples from '/home/dino/Documents/ia/controllers/cnn/cnn_dataset.h5'...\n",
      "Full dataset loaded. Shapes: Cam=(7493, 4, 40, 200), Lidar=(7493, 20), Target=(7493, 2)\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Fine-tuning for 10 epochs with LR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1/10 - Loss: 0.0028, MAE: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 2/10 - Loss: 0.0027, MAE: 0.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 3/10 - Loss: 0.0026, MAE: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 4/10 - Loss: 0.0026, MAE: 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 5/10 - Loss: 0.0025, MAE: 0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 6/10 - Loss: 0.0025, MAE: 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 7/10 - Loss: 0.0024, MAE: 0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 8/10 - Loss: 0.0023, MAE: 0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 9/10 - Loss: 0.0023, MAE: 0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 10/10 - Loss: 0.0022, MAE: 0.0292\n",
      "Fine-tuned model saved to: /home/dino/Documents/ia/controllers/cnn/final_cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "def fine_tune_model(best_checkpoint_path: Path, full_dataset_path: Path, fine_tune_epochs: int = 10, fine_tune_lr: float = 1e-5):\n",
    "    print(f\"\\n--- Starting Fine-tuning on Entire Dataset ---\")\n",
    "    print(f\"Loading best model from: {best_checkpoint_path}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the full dataset\n",
    "    (X_cam, X_lidar), y = load_data(full_dataset_path)\n",
    "    if X_cam is None or X_cam.shape[0] == 0:\n",
    "        print(\"Exiting fine-tuning: No data loaded for full dataset.\")\n",
    "        return\n",
    "\n",
    "    print(\n",
    "        f\"Full dataset loaded. Shapes: Cam={X_cam.shape}, Lidar={X_lidar.shape}, Target={y.shape}\")\n",
    "\n",
    "    # Create DataLoader for the full dataset\n",
    "    full_dataset = TensorDataset(torch.tensor(\n",
    "        X_cam), torch.tensor(X_lidar), torch.tensor(y))\n",
    "    full_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Instantiate model and load state dict\n",
    "    model = CNNNavigationModel(lidar_shape_in=X_lidar.shape[1]).to(device)\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Optimizer for fine-tuning (can be a new one with lower LR)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=fine_tune_lr)\n",
    "    # Optionally load optimizer state if resuming fine-tuning\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_criterion = nn.L1Loss()\n",
    "\n",
    "    print(f\"Fine-tuning for {fine_tune_epochs} epochs with LR: {fine_tune_lr}\")\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        running_loss, running_mae = 0.0, 0.0\n",
    "        progress_bar = tqdm(\n",
    "            full_loader, desc=f\"Fine-tune Epoch {epoch+1}/{fine_tune_epochs}\", leave=False)\n",
    "        for cam_batch, lidar_batch, target_batch in progress_bar:\n",
    "            cam_batch, lidar_batch, target_batch = cam_batch.to(\n",
    "                device), lidar_batch.to(device), target_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cam_batch, lidar_batch)\n",
    "            loss = criterion(outputs, target_batch)\n",
    "            mae = mae_criterion(outputs, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_mae += mae.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), mae=mae.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(full_loader)\n",
    "        epoch_mae = running_mae / len(full_loader)\n",
    "        print(\n",
    "            f\"Fine-tune Epoch {epoch+1}/{fine_tune_epochs} - Loss: {epoch_loss:.4f}, MAE: {epoch_mae:.4f}\")\n",
    "\n",
    "    # Save the final fine-tuned model\n",
    "    final_model_path = SCRIPT_DIR / 'final_cnn_model.pth'\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Fine-tuned model saved to: {final_model_path}\")\n",
    "\n",
    "if best_overall_checkpoint_path:\n",
    "    fine_tune_model(best_overall_checkpoint_path, DATASET_PATH)\n",
    "else:\n",
    "    print(\"\\nNo best model found for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Tamanho da característica da câmera achatada calculado dinamicamente: 24576\n",
      "Loading final model from: /home/dino/Documents/ia/controllers/cnn/final_cnn_model.pth\n",
      "\n",
      "Final Test Results:\n",
      "  - Test Loss (MSE): 0.0030\n",
      "  - Test MAE: 0.0293\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the test data if not already loaded\n",
    "if 'X_cam_test' not in locals() or 'X_lidar_test' not in locals() or 'y_test' not in locals():\n",
    "    print(\"Loading test data...\")\n",
    "    (X_cam_test, X_lidar_test), y_test = load_data(Path('/home/dino/Documents/ia/controllers/cnn/test.h5'))\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_dataset = TensorDataset(torch.tensor(X_cam_test), torch.tensor(X_lidar_test), torch.tensor(y_test))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNNavigationModel(lidar_shape_in=X_lidar_test.shape[1]).to(device)\n",
    "\n",
    "# Load the final fine-tuned model state\n",
    "final_model_path = SCRIPT_DIR / 'final_cnn_model.pth'\n",
    "if final_model_path.exists():\n",
    "    print(f\"Loading final model from: {final_model_path}\")\n",
    "    model.load_state_dict(torch.load(final_model_path, map_location=device))\n",
    "else:\n",
    "    print(\"Error: Final model 'final_cnn_model.pth' not found!\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_mae = 0.0\n",
    "criterion = nn.MSELoss()\n",
    "mae_criterion = nn.L1Loss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cam_batch, lidar_batch, target_batch in test_loader:\n",
    "        cam_batch, lidar_batch, target_batch = cam_batch.to(device), lidar_batch.to(device), target_batch.to(device)\n",
    "        outputs = model(cam_batch, lidar_batch)\n",
    "        test_loss += criterion(outputs, target_batch).item()\n",
    "        test_mae += mae_criterion(outputs, target_batch).item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_mae = test_mae / len(test_loader)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"  - Test Loss (MSE): {avg_test_loss:.4f}\")\n",
    "print(f\"  - Test MAE: {avg_test_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
